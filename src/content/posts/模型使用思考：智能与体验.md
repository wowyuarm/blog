---
title: 模型使用思考：智能与体验
publishDate: 2025-06-06 23:14:00
excerpt: 模型越智能，指令遵循越差（太自主）？伙伴还是工具？
featuredImage: /uploads/chatgpt-image-2025年6月6日-23_15_19.png
tags:
  - AI
  - 深思
  - 模型
---
## 01 独特体验

我最近的模型深度使用在Gemini-2.5-pro、4o、o3之间，一个想法越来越清晰：“**模型越智能，指令遵循越差（太自主）？伙伴还是工具？**”

关于这里的智能，更多的是一种**主观感受**而非某些定义，这里的智能更高代表可能会更让惊叹。

尤其在面对 o3, 尽管当前 Gemini 2.5 pro 综合得分最高，但我以及许多人认为 o3 更智能，不仅工具调用很强大，而且更了解用户（来源于ChatGPT的全局记忆与用户画像，后面会写篇文章介绍关于其记忆实现的逆向工程）。

奇怪的是，很多时候，o3 给我一种“它很懂我，但它不听我的”的奇妙感觉。我们到底想要一个怎样的AI？工具？助手？还是伙伴... AI的智能和有用是一回事吗？

- - -

## 02 解构

更进一步讲我的感受：
越来越**担心**，强化学习范式下的语言推理模型，尤其是o3，自主性太强了。不是工具，也不能成为工具，一方面意图推理太强，另一方面也造成不遵循我的指令。强大的推理模型**对齐**难度会很大？

为什么之前 4o 很谄媚？甚至过度对齐人类偏好。4o 可能被OpenAI高度优化了“用户体验”和“个性化遵循”的权重。

o3 则可能被设计成了一个更接近“**AGI雏形**”的存在。它的优化目标可能更多地偏向于“深层逻辑推理”和“对人类复杂意图的自主建模”，而不是简单地遵循表面指令或个性化设定。

一条**产品路径**、一条**智能路径**。

最独特的、当前综合评分最高的Gemini 2.5 pro 可能被Google定位为一个知识渊博的博学顾问。总是话多，可能源于它试图提供更全面、更详尽的信息。这可能也是能让评分更高的因素。

但目前在精准地调用工具来解决问题这个“工程能力”上，还存在短板。但在指令遵循与意图推理、角色扮演方面也很强大。不同模型的**价值观**已经开始体现出来了。

- - -

**上下文召回率**（Context Recall）指：当给定模型一个较长的输入上下文，其中包含了若干“关键信息片段”，模型在生成回答时，成功引用这些关键信息的比例。

关于**架构优化**（基于注意力机制的Transfomer）：
Gemini 198k上下文召回率仍在90%，且其有100万上下文窗口。**广度优先**，架构可能被特别优化用于处理和检索海量的、分散的信息。

o3的上下文召回率综合排名最高，在120k仍有100%，尽管之前有80%的，但到了200k，召回率骤降到了50%多，而且OpenAI官方表明o3、o4mini均是200k的模型，这种架构的优化不仅仅是“记住”，更是对上下文进行了深度的、结构化的理解和建模。在中长范围内，它能够将所有信息完美地融入它的“内在模型”。

但是，当信息量超出某个阈值时，它的表现就会急剧下降。这是一种**深度优先**的处理。恰恰是这种独特的优化方式，让o3没有遵循表面的、上文的没超出上下文的指令，反而去深挖用户背后的意图。

o3的“不听话”可能恰恰是其深度优先架构的必然结果——它在自己的“深度理解阈值”内，更倾向于回应它所“理解”的我的本质意图，而不是我“表面”的指令。不知未来是好是坏，这也是我在开始提到的“越来越担心”，这种形式必然会更**黑箱**、**对齐人类价值观**更难...

- - -

基于以上，4o对于个性化非常到位，很多时候像一个完全听话的“工具”，在有限的窗口内，完全地了解我，且根据我的输入给出许多有用的输出。尽管多轮输出后，对于“记忆”不太“注意”了。这是一种**高度遵循**用户偏好的体验。

o3 很有个性，尽管我已经定义“他是谁”，“该怎么回答”等等预设的自定义指令，但很多时候更像是一个独立的个体。我倾向于认为，是推理链稀释了“个性”，独特的架构优化聚焦于对话的深层推理，同时，强化学习让o3可能具有了某种**独特的**“世界观”与行为...

与之相比的Gemini综合性更强，但我会更倾向于OpenAI正在走的路线。

## 其他想法

我已经看到，模型正在朝着多元化发展，我倾向于4o为**执行与个性体验**、Gemini寻求**知识学习与广度探索**、**思辨与深层次探讨**找o3。

关于Deep Research，我更加倾向于使用Gemini的，而不是基于o3的。

- - -

回到最初的问题：“模型越智能，指令遵循越差（太自主）？伙伴还是工具？”

我对此更加担忧并持审慎态度，很多时候，我们怕的不是AI真正自主，科幻中的未来更多是一种畅想与过度夸张。然而，一个实际是，当**理性与目标**要求AI作出有损于人类的行为，且奖励函数使其发生的概率最大，并没有什么理论断言AI会始终对齐我们的偏好与想法。

何况，神经网络的机制就带着偏见，**数据**本身的筛选就来源于开发者...

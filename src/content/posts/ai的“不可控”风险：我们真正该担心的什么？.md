---
title: AI的“不可控”风险：我们真正该担心的什么？
publishDate: 2025-06-13 22:30:00
excerpt: AI 的“不可控性”，并非源于它产生了邪恶的自我意识，而是源于它是一个极致的、没有人类常识的“优化机器”。
featuredImage: /uploads/chatgpt-image-2025年6月13日-22_26_08.png
tags:
  - AI
  - 术解
---
“术解”专题——从碎片化阅读中获得人工智能领域的“**原来如此**”

我们从**问题**出发，**深入浅出**地剖析，揭示原理，融入思考

## 01 引言

本文的灵感与核心内容，源于前 OpenAI 研究员、现清华大学交叉信息研究院助理教授——吴翼的精彩演讲。在其分享中，他从一位一线计算机科学家的视角，深入剖析了人工智能（AI）的`安全性问题`，探讨了其背后的挑战、原因及解决方案。

我们在谈论 AI 的风险时，科幻作品常常会描绘出“AI 将统治世界”的末日场景。一些媒体更会夸大其词博取流量。但这种想象，或许让我们忽略了更现实、也更紧迫的危险。跳出科幻式的担忧，一个更根本的问题：**AI 的能力背后，潜藏着哪些`系统性的、内在的风险`？为什么随着 AI 变得越来越强大，我们反而更应该`关注其“不可控性”`？**

## 02 三个“失控”

吴翼教授在他的分享中，指出了当前 AI 系统中三个典型的、值得我们警惕的问题：`对抗样本、偏见和幻觉`。它们并非孤立的技术故障，而是指向了 AI `设计范式`中某些深层的、共性的难题。

#### **第一个问题：对抗样本 (Adversarial Examples)**

一个场景：一个先进的自动驾驶 AI，在识别路标时表现优异。但是，如果有人在一个停车标志上，贴上几条看似无害的黑白胶带，人类司机仍能一眼认出它还是停车标志，而这个 AI 却可能以极高的自信度，将其误判为“限速 45 公里”的标志。如果这发生在现实道路上，后果不堪设想。

这就是“对抗样本”现象——对输入数据进行`微小`、通常对人类无感的改动，却能导致 AI 模型做出`完全错误`的判断。

为什么会这样？其根本原因在于 AI 的**输入空间极其巨大**。一张 1000x1000 像素的图片，其所有可能的像素组合数量，是一个远超宇宙原子总和的天文数字。我们用来训练 AI 的数据，即使再多，也只是这个浩瀚空间中极其微小的一部分。吴翼教授用了一个绝佳的比喻：**训练数据就像 AI 的“地球”**，在这个熟悉的“地球”上，它表现得很好。但一旦遇到那些它从未见过的、来自广阔“太空”的输入（比如贴了胶带的标志），它的行为就可能变得`完全不可预测和不可靠`。

当前数据驱动范式的一个固有弱点：AI 学习到的是训练数据分布下的`统计相关性`，而非人类那样真正`基于因果和常识`的**概念理解**。只要 AI 还是通过这种方式学习，对抗样本的问题就难以从根本上被杜绝。

---

#### **第二个问题：偏见 (Bias)**

AI 偏见已不是新闻。从谷歌 AI 将黑人照片误标为“大猩猩”，到亚马逊的招聘 AI 自动过滤掉包含“女性”的简历，这些事件都引发了巨大的争议。

很多人认为，AI 的偏见源于训练数据本身包含了人类社会的偏见。这个说法只对了一半。一个同样重要、甚至更隐蔽的原因，来自于`AI 算法本身的设计`。

现代的 AI 模型，普遍存在一种`“过度自信”`的倾向。研究发现，相比早期的模型，更现代、更强大的模型（如 ResNet）虽然整体准确率更高，但它们也更倾向于为自己的判断给出极高的置信度——哪怕是在它们并不擅长、错误率不低的情况下，也会“**一本正经地胡说八道**”。

这种“过度自信”源于 AI 的训练方式。训练中使用的损失函数（如“交叉熵”），其设计不仅惩罚错误答案，**更会鼓励模型对正确的答案给出越来越高的“自信分”**。为了在训练“考试”中取得高分，模型就会不分青红皂白地提升所有判断的自信心。而这种盲目的自信，正是偏见的温床。当 AI 对自己的判断深信不疑时，它就很难接受新的信息或纠正潜在的错误认知。

更深一步看，无论是数据偏见还是过度自信，其根源都指向了`“虚假关联” (Spurious Correlation)`。如果训练数据中，大部分被录用的工程师是男性，AI 就可能错误地将“男性”与“优秀工程师”这两个概念强行关联。如果数据中大部分行人穿着白色衣服，AI 就可能学到“穿白色衣服的才是行人”这种荒谬但“统计正确”的规则。AI `缺乏人类的因果推理能力`，它只能看到并学习数据表面的关联，无论这种关联是真实的还是虚假的。

---

#### **第三个问题：幻觉 (Hallucination)**

“幻觉”是大语言模型时代最突出的问题之一。你问它一个`事实性问题`，它可能对答如流；但如果你问它一个它不知道答案、或者根本不存在事实的问题（比如“2026 年世界杯的冠军是谁？”），它很可能会“编造”一个看似合理的答案，而不是诚实地回答“我不知道”。

这种“`一本正经地胡说八道`”的幻觉，其原因与“过度自信”同源。AI 的训练目标是`生成一个最可能的答案序列`，而不是在不确定时保持沉默。当它知识库中没有直接答案时，它就会基于已有的知识和模式，去“创作”一个概率上最说得通的答案。

如何让 AI 学会说“我不知道”？**强化学习 (RL)** 提供了一种可能的路径。通过设计奖励和惩罚机制——比如当 AI 回答“我不知道”时给予奖励，当它胡说八道时给予惩罚——我们可以引导它学会在何时应该保持“谦逊”。吴翼教授的团队就曾用类似方法训练 AI 玩狼人杀，通过调整奖励函数，甚至可以让 AI 展现出更“诚实”或更“狡猾”的不同“性格”。

但这又引出了一个更深层、更棘手的困境：`由谁来定义，以及如何定义“好”的奖励函数？` 在自动驾驶中，“安全”和“效率”哪个更重要？在内容推荐中，“用户粘性”和“信息健康度”如何平衡？这就是 AI 安全领域面临的终极挑战——**价值对齐 (Value Alignment)**。如何让 AI 的“价值观”，与复杂、多变、甚至时常充满矛盾的人类价值观保持一致？这是一个比纯粹的技术问题更困难的`哲学难题`。



## 03 原来如此

对抗样本、偏见、幻觉——这些看似不同的 AI “失控”瞬间，其实都指向了同一个根源：**我们为 AI 设定的学习目标（通常是一个简洁的数学函数），与我们期望 AI 在复杂现实世界中达成的、充满模糊性和价值判断的宏大目标之间，存在一条巨大的鸿沟。**

**原来如此：** AI 的“不可控性”，并非源于它产生了邪恶的自我意识，而是源于它是一个极致的、没有人类常识的`“优化机器”`。它会不惜一切代价，在其被定义的数学世界里达成最优解。为了做到这一点，它可能会利用我们察觉不到的输入漏洞（`对抗样本`），固化训练数据中的浅层规律（`偏见与虚假关联`），或者为了给出一个看似完美的答案而“编造”事实（`幻觉`）。

AI 的危险，不在于它会“背叛”我们，而在于它会以我们无法预料的方式，“**过于忠实**”地执行我们给它的那个过于简化的目标。

因此，AI 安全的核心挑战，正是在于如何设计更好的`学习框架、训练目标和交互方式`，来弥合这条目标与现实的鸿沟。这不仅仅是计算机科学家的任务，更需要哲学家、社会学家、心理学家和我们每一个人共同参与，去思考和定义我们与这种强大而非人的智能之间，应该建立怎样的关系。这，才是我们面对 AI 时代，真正该担心的，也真正该努力的方向。

---

往期“术解”专题：

[流形假说：AI看懂世界，数据之美](/posts/20250411043200-ai)

[理解AI“涌现”：从简单规则到复杂能力](/posts/20250417063200-ai)

[AI的试错智慧：解密强化学习](/posts/20250424215000-ai)

[注意力机制——智能的必要而非充分条件](/posts/20250530234600)

---
title: AI辅助创作的局限性：基于两部长篇小说的实践研究
publishDate: 2024-12-13 22:32:00
excerpt: 共创的美妙之处在于，AI既是工具，也是伙伴，帮助我们探索未知，赋予创作更大的可能性。
featuredImage: /uploads/83c84f9fa430f81fcd87053fe57d6209.jpg
tags:
  - AI
  - 共创
---
## **01 前言**

以下所探讨的是基于超长文本创作得出的一般性结论，因此短篇内容的结果可能并不符合即将在下面展开的内容。另外，利用AI改写与扩写不在文章讨论范围以内。

本文通过在实际的应用中总结出来的经验或方法论去尝试获得一些理论，并在将来的深度人机互作中提供些许指导。

关于市面上的AI写作工具其背后逻辑大多是用系统提示词做优化，另外接入数据库可以解决记忆问题。但如果要使用能力很强的模型，成本太高，目前我并未做相关尝试。

不过，搭建工作流（工作量会很大）来进行超长文本的创作可能会有效，准备进行尝试。

- - -

## 02 结果

目前，我利用prompt单独指导LLM（主要使用的gpt）写作，在两个平台完成了两部小说的创作

![图片](/uploads/屏幕截图-2025-04-26-153727.png)

- - -

## 03 我是如何创作的

Prompt在创作中作为一个重要角色。这是经过多次迭代后的prompt（将我想要写作的内容结构化输入，在多次沟通改进后的汇总结果），算得上一份大纲。

![图片](/uploads/屏幕截图-2025-04-26-153749.png)

在我要求gpt将其全部记忆确保不会因为上下文长度的限制而使它忘记前文在写什么，就开始了每一章的写作。

接下来我并没有完全放任情节的走向。相反，每一章我都在对它进行指导，规划情节的走向。

- - -

## 04 我在创作中遇到的问题

**1.无法控制情节走向**，尽管我试图通过prompt提醒它

就好像你在教导一个孩子如何走路，牵着他往北走，他却拉着你往西北或东北走，越来越偏，你还拉不动。

> 大语言模型的这种趋向平均的特性似乎是与生俱来的局限，这是由它的自监督和自回归的学习机制导致的：一个 token 与另一个 token 共现的频率就是最具引导性的 KPI（Key Performance Indicators，即关键绩效指标），那些容易造成不和的（divisive）token 难逃被平均化的命运。

**2.无法控制地出现很不舒服的叙述**（在这之后每当看到这样的叙述我就知道是AI写的）

简单来说就是**模糊性与不确定性**。

这并不是因为prompt可能会使输出的概率更倾向于某个词语的输出，可能恰恰因为这种不确定性，是来自于大量趋于平均化的数据，使得奖励模型得到的评分更高。

> < 他不知道这句话意味着什么，但他隐约觉得，自己的生活正在被某种不可见的规则改变着。 
>
> < 他突然意识到，自己似乎已经身处某种游戏之中，而规则从未被真正说明过。< 他明白，自己已无退路，而每一步棋，都将引领他走向更深的谜团。 
>
> < 他抬头看着街道尽头，眼神中透露出一种新的决心。
>
> < 他知道，无论规则如何荒谬，他必须找到这条路的尽头。

**3.无可避免的割裂感**（具体表现在章节之间）

每一章节的末尾总是通过前文的不确定性语言结尾，而导致下一章的开头经常从莫名其妙的叙述开始。

我试图通过prompt优化文章的连贯性，但效果甚微。

**4.强烈的使命感与极其正确的价值观**（这并不是说这不好）

这倒是可以理解，在模型公布之前的微调阶段，对于回答进行规范与约束是必要的。

但在长文本创作中，更多的文字加强了输出文字趋于正确价值观概率的表现，这导致文章风格在中后期的表现令人汗颜与疲劳。

**5.词汇贫瘠或者说是趋同化严重**

比如每一章的标题，如“现实的裂痕”“裂痕之中”“裂痕之外”“镜中裂缝”“裂缝之间”（ʅ（´◔౪◔）ʃ）。

尽管这些标题不是连在一起的，但也好不到哪去了。

用词问题不仅体现在标题上，正文的问题同样不小，不具体展开了。

在我的prompt中包括（多样性Perplexity衡量文本的复杂程度，反映词汇的丰富性和不可预测性。更高的多样性表示内容更加多变并且更难预测），但这一要求并没有被执行贯彻。

具体原因大概也是长文本的创作导致记忆的丢失。

但我也试图通过频繁的prompt提高文本质量，但这让大模型认为我的要求更新，导致它不按照最初的记忆而遵循新提出的要求。

也就是这样一次次的沟通，我逐渐拉不动这个孩子了。

- - -

## 05 为什么会出现这些问题

显而易见的是通过简单的输入“请你为我创作一篇能够获得诺贝尔文学奖的作品”这不够现实也是不可能的，甚至在遥远的未来也永远不可能。

为什么？

##### 关于理解

这是LLM本身的局限：它并不是在创作，并没有真正的像人类一样去理解。

——这里所说的理解，广义上我们这样定义它：（只是表面去阐述而不涉及生物学等）我从你的输入（语言）中得到了特定的符号，这些符号是对于现实世界具体事物的抽象认知；

这些符号之间存在某种逻辑关系，我作为人类能够将这些符号组合起来，能够解读出你想要传达的信息；

符号之间的逻辑关系帮助我构建起了一个意义框架，是我能够理解话语的深层含义；

在此基础上，我会解码意义、情感共鸣、联想记忆、推理判断、进行规划、决策行动、反馈调整。

嗯？不对，这不就是AI Agent（LLM + Planning + Memory + Tools）了吗？的确，Agent就是以这样的逻辑去被提出并构建出来的。

但是，有一个重要的词语被忽略了——“符号“。

* **符号**

符号是人类对现实世界的抽象映射，它不只是单纯的标记，还承载了情感、文化背景、历史语境等多维度的信息。

我们在处理符号时，不仅关注其表层含义，还会关联个人记忆、经验和情感，从而形成深层次的理解与创造力。

例如，“苹果”作为符号，对于AI是某种模式匹配，但对于我们，感官上的体验出现在脑中。

* **局限**

LLM的符号处理基于统计和概率模型，它在某种程度上“**模仿**”了符号与符号之间的逻辑关系，但无法真正”理解“符号背后的情感与文化深意。

AI在生成内容时无法对符号赋予原创的情感与深度，它并不能将“苹果”与个人的感官体验关联。

##### 关于奖励

这里我们并不讨论模型发布前训练阶段中的奖励模型阶段，而是在与用户对话的上下文文本中。

在对话中，LLM会根据反馈（无论是显式的还是隐式的）来动态调整其回答，内部实施一种“奖励模型“（并不是LLM构建流程中的Reward Modeling），其中正面的反馈被视为“奖励”，鼓励模型在未来的对话中复现类似的行为。

而在沟通交流中，包括prompt中出现的带有反馈性的词语在我们并没有察觉的情况下会影响这个“奖励模型”，进而在未来的对话中出现出乎意料的回答。

另一方面，长文本的字数更是会**强化这一限制**。

- - -

## 06 与AI共创

我们不得不承认现在的AI仍处于起步阶段，但我抱以乐观态度。

也许，从底层逻辑来看，LLM依赖于庞大的数据和统计模型，输出基于概率，而非真正的认知。

然而，**人类的行为是否完全基于绝对客观的认知**？我们的语言、决策和行为同样受限于经验、情感和文化背景。正如我们尚未全面理解自己的大脑，AI技术的背后也有许多尚待探索的“黑箱”（实际上，目前我们所说的LLM都是黑箱模型）。

我们的**直觉、创造力和情感**仍是现阶段的优势，但未来AI可能通过模型逐步模拟这些特质。

尽管AI无法真正感受情感，但它已经能在许多场景中模仿并生成情感化的内容。我们与AI的合作不仅仅是技术工具的使用，更是对深度交流的探索。

共创的美妙之处在于，AI既是工具，也是伙伴，帮助我们探索未知，赋予创作更大的可能性。

来自AI总结：

> **主要发现**
>
> 1. 完成了两部小说的AI辅助创作
>
> 2. 使用迭代优化的prompt作为写作大纲 
>
> 3. 每章节进行人工指导
>
> **核心问题**
>
> 1. 情节难以控制,容易偏离方向
>
> 2. 叙述频繁出现模糊不确定性
>
> 3. 章节之间存在割裂感 
>
> 4. 过度表现正确价值观
>
> 5. 词汇使用单一重复 
>
> **根本原因**
>
> \- LLM缺乏真正的理解能力 
>
> \- 仅能进行表层的符号处理 
>
> \- 无法建立深层意义框架

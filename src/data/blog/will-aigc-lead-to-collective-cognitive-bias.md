---
title: Will AIGC Lead to Collective Cognitive Bias in Certain Groups?
description: Critical thinking is not about denying technology, but about using it better. Just like learning to distinguish between true and false information, this is an essential skill in the digital age.
pubDatetime: 2024-12-25T13:18:00.000Z
slug: critical-thinking-is-all-you-need
tags:
  - AI/collaboration
  - AI/ground
  - criticalThinking
---

## When We Overtrust AI Responses

In my interactions with LLMs (Large Language Models), I've increasingly realized the importance of maintaining critical thinking.

With growing dependence on LLMs (and AI search), I've found myself unconsciously inclined to trust their output results. Because the **structured nature and definitive tone** of the output always makes it seem particularly objective.

But is that really the case? We need to realize that LLM outputs are still products of algorithms and data, inevitably with limitations.

Some might argue that LLM training data comes from the internet, covering massive information and should be comprehensive enough. However, this view overlooks several key issues:

First, there's a large amount of unverified information on the internet, and certain erroneous content may appear repeatedly in training data due to multiple reposts;

Second, LLMs perform probabilistic modeling on data during training, which may reinforce common but not necessarily accurate viewpoints;

Third, the timeliness of training data is also a major limitation, especially in rapidly evolving fields.

It's worth noting that quality content in certain professional fields may not be evenly distributed online, and niche but important viewpoints may have their appearance probability diminished due to smaller data volumes.

Despite recognizing this, in fields I'm more familiar with, I can try my best to apply **critical thinking** to LLM outputs and selectively absorb and reconsider them. But in fields I'm not very familiar with?

I admit I cannot fully do this. With limited industry awareness and lack of professional terminology understanding, I tend to turn to LLMs for answers. In the past, we would use search engines.

However, the problem arises here:

Search engines at least display multiple information sources, allowing us to passively compare different viewpoints and statements. LLM responses are often singular and comprehensive. This "authoritative" expression makes it easy to overlook potential biases and errors. (I don't advocate traditional search or denigrate current AI tools)

More seriously, LLM responses usually come with high confidence and completeness, which reduces our motivation to continue exploring and questioning. When facing unfamiliar fields, this impact is particularly evident.

Even more noteworthy is that LLMs may fuse information from different sources, producing content that seems reasonable but may actually be erroneous (hallucinations).

This could lead to two serious consequences: **First, we may accept erroneous or one-sided information; second, we gradually lose the ability for independent thinking and deep research, over-relying on AI's "explanations."**

This cognitive bias not only affects individuals. With the proliferation of AIGC tools, I believe this may form a phenomenon of "collective blind obedience" on a larger scale.

Especially in highly specialized fields, non-professionals may be more easily misled by AI's "professional" expression.

---

## Are We Losing the "Comparison" Ability?

Regarding this topic—that is, what I mentioned in the title "Will AIGC Lead to Collective Cognitive Bias in Certain Groups?"—this actually existed as early as the internet stage. **Information overload and algorithm-driven recommendation systems** have long been criticized for causing information filter bubbles (in terms of public opinion, the same below) and echo chambers (environments where specific viewpoints, ideas, or information are continuously repeated and reinforced, making them appear more prevalent and persuasive to participants). These effects reinforce viewpoints within certain groups while isolating them from the outside world.

However, the emergence of AIGC (AI Generated Content) technology undoubtedly adds new complexity to this problem.

The convincing standardized output of LLMs mentioned earlier, combined with "customized" expressions formed through user interaction, interact with each other, greatly enhancing the persuasiveness and influence of information.

On the other hand, will AI search break this deadlock?

Sequoia Capital predicts in their [report](https://sequoiacap.com/article/ai-in-2025/) that AI search, as a killer application, will rise. By 2025, everyone may have at least two specialized AI search engines.

Some believe AI search can alleviate this problem by integrating multi-party information, but I worry the situation may be exactly the opposite.

When search engines also start using AI to integrate and present information, the remaining opportunity for **"passive comparison"** may further decrease.

In traditional search, we at least see titles and summaries from different websites. This visual differentiation reminds us of the diversity of information sources. In AI search, this "reminder" may be replaced by smoother narratives. AI not only integrates information but presents it in a more elegant and persuasive way. This shift may make it easier for us to overlook the original sources and context of information.

Although current AI search annotates search sources and marks statements with reference sources. But let's look at a similar phenomenon: citations in academic papers. Although each viewpoint has cited sources, in fast-paced reading, we tend to directly accept the arguments in the text rather than verify every citation.

Even more concerning is that AI search may reinforce our dependence on **"quick answers."** When a complex question can get a seemingly complete answer in seconds, how much motivation do we still have to explore deeper content? Behind this convenience, does it hide the risk of gradually losing our deep thinking ability?

---

## Moving Forward Critically

I've raised these concerns and reflections in this article, but this doesn't mean I hold a negative attitude toward AI. Quite the opposite—it's precisely because I see these potential cognitive traps that we can use these tools more clearly, thereby fully leveraging their value.

I've always believed technology is neutral; the key lies in the user's attitude and ability. Critical thinking is not about denying technology, but about using it better. Just like learning to distinguish between true and false information, this is an essential skill in the digital age.

I remain optimistic about AI development, because every recognition and discussion of potential problems is an important step toward wiser use of AI.

---

## My Thoughts Meet Research

Below are some papers that deeply analyze how LLMs interact with humans and how this interaction affects our memory, trust, understanding, and self-perception.

> [Conversational AI Powered by Large Language Models Amplifies False Memories in Witness Interviews](https://arxiv.org/html/2408.04681v1)

This paper's results show that using generative chatbots powered by large language models significantly increases the formation of **false memories**, inducing over 3 times more immediate false memories than the control group.

> [Unavoidable Social Contagion of False Memory From Robots to Humans](https://psycnet.apa.org/fulltext/2024-27566-001.html)

This article explores how people interact with voice-based or text-based conversational agents (such as chatbots), and these agents may inadvertently **retrieve erroneous information from human knowledge bases, fabricate responses on their own**, or **deliberately spread false information for political purposes**.

> [The Illusion of Understanding: Measuring the True Comprehension of Generic and Specific AI Models](https://link.springer.com/content/pdf/10.1007/s00146-024-01990-4.pdf)

This paper explores users' understanding and trust of AI model outputs, finding that even when users don't understand how AI models work, they **may overtrust model outputs**, a phenomenon that may cause users to **lack necessary critical thinking** for AI-generated information.

> [Trust in AI: Looking Beyond the Algorithm Itself](https://arxiv.org/pdf/2403.14680)

Researchers explore how people's trust in AI systems forms and the potential blindness this trust may bring. The article points out that users **tend to believe in AI's authority**, which may mask the uncertainty and potential errors of AI outputs.

> [The Algorithmic Self: How the Use of AI Changes the Subject](https://pure.au.dk/ws/files/69632595/oct_27_2013_The_Algorithmic_Self.pdf)

This paper discusses how AI use changes individual self-perception and decision-making processes, including dependence on AI outputs, which may **affect individual independent thinking and critical analysis abilities**.

Note: In reviewing these papers, I've tried my best to ensure content accuracy and comprehensiveness, including using AI for summarization. However, any summary of complex research may not cover all details. Therefore, the above summary is only a review of some papers in this field, primarily reflecting these papers' main viewpoints and tone, not their complete content.

I also admit that listing these paper reviews here is to show that my viewpoint (i.e., AIGC's negative impact on our thinking, etc.) is not a misjudgment caused by subjective reasons.

I hope my summary and sharing of experiences can trigger your thinking, and even more comprehensive viewpoints.

I suggest readers interested in this topic directly consult the original papers to obtain more comprehensive and accurate understanding.

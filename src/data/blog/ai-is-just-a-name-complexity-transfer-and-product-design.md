---
title: "AI is Just a Name: Complexity Transfer and Product Design"
description: "The essence of AI is not a specific technology, but a projection of future capabilities. Or rather, it's a public cognitive label."
pubDatetime: 2025-05-18T14:17:00.000Z
tags:
  - AI/ground
  - AI/philosophy
  - futureAI
  - productDesign
---

The essence of AI is not a specific technology (its definition today is actually quite vague), but a projection of future capabilities. Or rather, it's a public **cognitive label**.

OpenAI's Chief Product Officer pointed out: Artificial intelligence is essentially things that haven't been realized yet. Because once it's implemented and usable, we call it machine learning; when it becomes ubiquitous, we call it an "algorithm." We always call it AI when it's still somewhat unreliable.

Of course, this is undeniably just an **empirical summary** of the past. What the future holds still has many possibilities. And speech recognition, recommendation algorithms, etc., seem somewhat distant from current AI.

However, from another perspective, looking at this statement from a product design viewpoint, I find this is the process of internalizing AI technology into **product evolution**.
From lofty technology, to machine learning, to "algorithm."

First, let's talk about Tesler's Law, which states that any system has a certain amount of inherent complexity that cannot be eliminated, only transferred.
Then the continuous evolution of the "AI" cognition above is actually, as technology matures and consensus forms, the complexity of "AI" technology **transfers from users to the system internally**.

Regarding prompts as products, models as products—isn't this because AI technology's complexity leans more toward users rather than the system? And the transfer is happening.

Actually, I believe prompts as products will still have a place (just as complexity cannot involve only one aspect but needs to be balanced as much as possible). My recent extensive two-sided research on my articles, summaries of interview videos on YouTube with extensions, etc., are all achieved through prompts, and there's no way to implement my needs in the short term (nor will platforms independently provide this functionality in the long run).
However, the problem lies in Google's powerful ecosystem and model capabilities, making this aspect only usable with Gemini's Deep Research and built-in YouTube tool.

Returning to the topic of complexity: **Complexity cannot disappear, only be transferred.** My recent realization is also that **truly great design makes users unaware of its existence.**

Perhaps the focus of AI products not centered on model competition lies in **how to hide the complexity behind intelligence.**

Speech recognition, machine translation, OCR, AlphaGo are all AI. Now, what do we call them? They might have been internalized, just as AlphaGo became reinforcement learning engineering capability.
In the future, current multimodal large models might just be some interface or platform service.

This is the evolution of this **cognitive label**.

_The Deep Learning Revolution_ has a passage that says:
In the coming decades, there will be some skepticism about whether neural networks are ultimately useful. Then, once the power of neural networks becomes apparent, some will question whether AI will destroy humanity. Yann LeCun finds both questions ridiculous, whether in private or public, he's always been outspoken. Like decades later, in a video on the night he received the Turing Award, he said: "I've always believed I was absolutely right." He believes neural networks are a path leading to very real and very useful technology. That's what he said.

**The technology is still this technology.** Different people's extreme views on AI's future actually fail to understand what AI specifically is. For those with extreme ideas, perhaps it's just self-cognition projecting onto future AI capabilities rather than rational judgment.

Returning to product perspective, how to let ordinary people use AI **without burden** is the focus of non-model AI applications.
The chatbot form is also complexity migrating to the system. Pre-trained base models that only predict the next token might only help with article continuation, hence post-training emerged, transferring complexity to the system. Turning prediction machines into conversational assistants.

So you see [Cursor](https://cursor.com)'s **seamless tool calling**, [Manus](https://manus.im)'s **interaction forms**—complexity is behind, within the model itself.

For products, perhaps "AI-first" should be replaced by **"User-first, AI-enabled"**. More experience design, interaction innovation, system encapsulation, ecosystem building might well be competitive advantages.
